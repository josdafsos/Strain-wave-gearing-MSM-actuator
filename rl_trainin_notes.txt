Simple MLPs with two or three layers can maintain a control with a fixed setpoint

For random setpoint system was tested
        net_arch=[256, 256, 512],  # hidden layers with VALUE neurons each
        activation_fn=torch.nn.ReLU
for 3e7 steps. Not converged, no progress

---------------
DQN training:
Architecture:
        net_arch=[256, 256],  # hidden layers with VALUE neurons each
        activation_fn=torch.nn.ELU

        model = DQN("MlpPolicy",
                    vec_env,
                    device=device,
                    learning_rate=learning_rate,
                    policy_kwargs=policy_kwargs,
                    gradient_steps=-1,  #-1,  # suggested by Ming, default 1
                    batch_size=256,
                    verbose=1,)

simulation_time=0.03
Reward = max(1 / (0.1 + abs(dv)) - 9.8, 0)
Velocity, velocity error and velocity error derivative are not normalized
Trained for 2e6 steps, learning rate 1e-4

1. 10 stack; obs: ['rack_phase', 'rack_vel', 'rack_tanh_acc', 'velocity_setpoint',],
Final mean reward (several runs): oscillate around 7.0; around 5.0; around 9.0

2. 10 stack, obs: ['rack_phase', 'rack_vel', 'rack_tanh_acc', 'velocity_setpoint', 'error_integral', 'error_derivative', 'is_max_teeth_engaged']
Final mean reward: oscillation around 11.0 (not yet converged to maximum), around 10.7; reward peaked at around 11.0 (during first 40% of training) and then degraded to 2

3. 5 stack, obs: ['rack_phase', 'rack_vel', 'rack_tanh_acc', 'velocity_setpoint', 'error_integral', 'error_derivative', 'is_max_teeth_engaged']
Final mean reward: 11.1, not converged to maximum yet; 11.5; scillating around 9.46

4. obs + 4*stack: core obs ['rack_phase', 'rack_vel', 'rack_tanh_acc', 'velocity_setpoint', 'error_integral', 'error_derivative', 'is_max_teeth_engaged']
+ 4 x stack: ['rack_phase', 'rack_vel', 'rack_tanh_acc', 'velocity_setpoint',]
Final mean reward: 11.6 not yet converged; 10.7 almost converged; 10.4

5. obs + 10*stack: core obs ['rack_phase', 'rack_vel', 'rack_tanh_acc', 'velocity_setpoint', 'error_integral', 'error_derivative', 'is_max_teeth_engaged']
+ 10 x stack: ['rack_phase', 'rack_vel', 'rack_tanh_acc', 'velocity_setpoint',]
Final mean reward: 10.6 not yet converged; 9.7; oscillating around 8.0

6. obs + 6*stack: core obs ['rack_phase', 'rack_vel', 'rack_tanh_acc', 'velocity_setpoint', 'error_integral', 'error_derivative', 'is_max_teeth_engaged']
+ 6 x stack: ['rack_phase', 'rack_vel', 'rack_tanh_acc', 'velocity_setpoint',]
Final mean reward: 11.2 not yet converged; 10.6; 9.9

NOTE: the following trials have all normalized input data

7. obs + 5*stack: core obs ['rack_phase', 'rack_vel_normalized', 'rack_tanh_acc', 'velocity_setpoint_normalized', 'error_integral', 'error_derivative_tanh', 'is_max_teeth_engaged']
+ 5 x stack: ['rack_phase', 'rack_vel_normalized', 'rack_tanh_acc', 'is_max_teeth_engaged']
Final mean reward: 10.8; 11.4 not fully converged; high oscillations from 6.27 to 11.7

8. obs + 5*stack: core obs ['rack_phase', 'rack_vel_normalized', 'rack_tanh_acc', 'velocity_setpoint_normalized', 'error_integral', 'error_derivative_tanh', 'is_max_teeth_engaged']
+ 5 x stack: ['rack_phase', 'rack_vel_normalized', 'rack_tanh_acc', 'is_max_teeth_engaged', 'velocity_setpoint_normalized']
Final mean reward: 10.6; 12.8 not fully converged; 11.2

8-1:
12.8 post-trained for 2e8 steps,with velocity setpoint_limits=(0.006, 0.010), learning_rate = 1e-8 and simulation time 0.06. Resulting mean reward 20.6 (10.3 recalculated for 0.03 sim time). 1e8 steps computed for 1d, 3 hours.
DQN has better performance than pid at velocity 0.0076

Architecture modified to:
    policy_kwargs = dict(
        net_arch=[16, 256],  # hidden layers with VALUE neurons each
        activation_fn=torch.nn.ELU
    )

9. Obs + stack same as in 8.
Final mean reward: 7.8; 8.0; 8.0

10. Obs + stack same as in 8. net_arch=[32, 256],
Final mean reward:peak with 8,0, oscillate around 7.0, at the end dropped to 5.0, but before reached 10.0

11. Obs + stack same as in 8. net_arch=[64, 256],
Final mean reward: 10.1 not fully converged; 10.1 not fully converged (yes, again);12.6 not fully converged

12. Obs + stack same as in 8. net_arch=[80, 256],
Final mean reward:degraded to 6.8, peaked at 11.4; 10.4 not  converged; 12.4 not converged yet

12. Obs + stack same as in 8. net_arch=[100, 256],
Final mean reward: 12.4 not converged yet; 10.9 not fully converged

13. Obs same as in 8. architecture:
policy_kwargs = dict(
        net_arch=[256, 256, 256],
        activation_fn=torch.nn.ELU
    )

simulation_time=0.06, velocity set point limits=(0.006, 0.010)

Mean reward after3e7 episodes 16, not converged yet, lr= 1e-8.
LR reduced to 1e-9, training continued - no progress observed after 1e7 steps.
LR changed to 1e-6 much better performance, reward started to grow and reached 16.5 (not converged yet), trained for 5e6 steps
LR changed to 1e-3: very poor performance, the agent rapidly degrades
LR changed to 1e-4: the max reward reached over 18.0, but the training is unstable and reward jumps back and forth
LR changed to 1e-4: 1e-5. trained for 5e6 steps, reward 21.6, not converged yet
Training continued for 2e7, reached reward 22.3, but at the end dropped to 19.7 (use a checkpoint from the middle)
Checkpoint with 44000000 steps worked best most of the times (on each training run the result is different), training continues from it.
The velocity range was expanded to full available range: velocity set point limits=(0.001, 0.012)
With RL 2e-5: the system degraded
With RL 5e-6: initial reward was poor due to set point range extension, but it converged back to 21.3; training continued for 5e7, peak reward over 24.2 at 97M steps and 98.5Mcsteps converged.
Training continued from 97M steps, With RL 1e-6, for 5e7 steps: reward 23.9, converged
Training continued, With RL 5e-7: No improvements since the previous run

14. New architecture:
    policy_kwargs = dict(
        net_arch=[512, 512],
        activation_fn=torch.nn.ELU
    )
    RL = 1e-6
    simulation_time=0.06, setpoint_limits=(0.001, 0.012)
    Run for 3e7 steps, 30% of steps for exploration (epsilon decay to 5% when 30% of steps is reached)
    Reached up to 20.0, converged at 1e7 steps
    Training continued for 1e7 with LR=5e-6 - reward increased to 21.4 and converged
    Training continued for 1e7 with LR=1e-5 - converged to 22.5
    Training continued for 1e7 with LR=1e-6 - converged to 23.9
    Training continued for 1e7 with LR=2e-7 - not much improvement, oscillating around 24.0; NAMED AS "stable_24_8_fit_dqn_32_obs_4000_Hz_freq_30000000_steps"
    Training continued for 1e7 with LR=5e-8 - did not improve, retraining without this episode
    Training continued for 1e7 with LR=1e-8 - did not improve, retraining without this episode, yet 25.1 reward was noticed at some point (very narrow time period)
    Training continued for 3e6 with LR=2e-6 - did not improve, retraining without this episode
    Training continued for 3e6 with LR=5e-7 - slightly improved to 24.1 (even though previous recording was higher)
    Training continued for 3e6 with LR=5e-7 - did not improve, retraining without this episode
    Training continued for 3e6 with LR=2e-7 - did not improve, retraining without this episode
    Training continued for 3e6 with LR=2e-7 - improved to 24.7, named stable2_dqn_32_obs_4000_Hz_freq_30000000_network
    On average better that PID

15. Training for a variable force: idea is to train two networks for vel range 6 - 8 and 8 - 10 mm/s, but both optimized for force and use corresponding NN for the desired Velocity setpoint
MSM_Environment(simulation_time=0.06, setpoint_limits=(0.008, 0.010), force_limits=(-8, -1), action_discretization_cnt=20)
 policy_kwargs = dict(
        net_arch=[512, 512],
        activation_fn=torch.nn.ELU
    )
 RL = 2e-6
 Converged to 17.3 after 40% of the training, named as force_training_15_experiment_dqn_32_obs_4000_Hz_freq_10000000_steps
 Training continued for 3e6 with LR=5e-6, did not improve, retraining without this episode
 Training continued for 3e6 with LR=1e-6, did not improve, retraining without this episode
 Training continued for 3e6 with LR=2e-7, slightly improved
 Training continued for 3e6 with LR=1e-7,

16. (simulation_time=0.06, setpoint_limits=(0.001, 0.010), force_limits=(-7, -1), action_discretization_cnt=20)
Training continued for 1e8 with LR=1e-6
    policy_kwargs = dict(
        net_arch=[256, 256, 256],  # hidden layers with VALUE neurons each
        activation_fn=torch.nn.ELU
    )
Epsion converges to 0.05% after 30% of steps (previously in all experiments was 20% of episodes)
Worked well, learning curve saved in documents. end reward 23.4; max found reward (from console) 25.1

17. same as 16 once again, te-training
converged after 6e+7 steps, average reward around 22.0

18. Newtopology, other parameters unchanged
    policy_kwargs = dict(
        net_arch=[512, 512],  # hidden layers with VALUE neurons each
        # activation_fn=torch.nn.ReLU
        activation_fn=torch.nn.Tanh
    )

Converged After 30% of steps. average reward around 23.5

19. "best_dqn_32_obs_4000_Hz_freq_97000000_steps" was trained for 1e+6 steps with 3% chance of zero velocity set point and
for other 97% set point was in [0.000, 0.012] m/s range.
Reward changed from 11.7 to around 22.5 (under new conditions), oscillations have smaller frequency, but the positioning error higher than the benchmark DQN + PID

20. Retraining same as in 19. 1e+7 steps, RL 1e-6
Converged at about 20% of the steps, mean reward around 26.0

------------------------------------
Training SAC
------------------------------------


1.         net_arch=dict(pi=[256, 256],
                      qf=[256, 256]),  # hidden layers with VALUE neurons each
                        activation_fn=torch.nn.ELU)
        Batch size=256, other settings are default

        LR = 1e-6, Steps 3e6
        Converging to average reward of 3.5

2. Added action noise                     action_noise=NormalActionNoise(np.array([0.0]),  # mu
                                                   np.array([0.1])),  # sigma
       Critic topology changed to match DQN critic topology
       policy_kwargs = dict(
        net_arch=dict(pi=[256, 256],
                      qf=[256, 256, 256]),  # hidden layers with VALUE neurons each
        activation_fn=torch.nn.ELU

        LR = 1e-7, Steps 3e6
        Reward converged to around 3.9

 2.1 Tested other noise values, they do not seem to help at all, thus noise is excluded (exploration in SAC is achieved by entropy anyway)

3. No action noise, same topology as in 2. LR=1e-3. Reward reached 12.5, not converged
continued for 1e6 steps, reward improved to 13.3 and converged
continued for 1e6 steps, RL=1e-4 reward improved to 15.8 and converged
continued for 1e6 steps, RL=1e-5 reward improved to 16.4 and converged, peak at 17.2
continued for 1e6 steps, RL=5e-5 reward degraded

4. Simplified Actor architecture
    policy_kwargs = dict(
        net_arch=dict(pi=[256, ],
                      qf=[256, 256, 256]),  # hidden layers with VALUE neurons each
        activation_fn=torch.nn.ELU)
    LR=1e-4 for 3e6 steps, reward 6.3, not converged. Requires a lot of time to converge

5. Changed Actor architecture:
    policy_kwargs = dict(
        net_arch=dict(pi=[128, 128],
                      qf=[256, 256,]),  # hidden layers with VALUE neurons each
        activation_fn=torch.nn.ELU)
    RL=3e-4, Steps =3e6. Reward converged to 6.8

6. More complex topology
        net_arch=dict(pi=[256, 256, 256],
                      qf=[256, 256, 256])
  RL=1e-3, 3e6 steps. Seems to converge around 11.0
  Continued for 3e6 steps, LR=1e-4. Reward converged around 12.0

  Run for 3e7 steps, LR=1e-5, reward oscillates around 13.0 (converged), peak was around 16.0

7. New topology, LR =1e-5
  policy_kwargs = dict(
        net_arch=dict(pi=[256, 256, ],
                      qf=[256, 256, 256]),  # hidden layers with VALUE neurons each
        # activation_fn=torch.nn.ReLU
        activation_fn=torch.nn.ELU
    )
  1e+7 steps, reward 3.0, converged

8. Same topology as in 7.
gradient_steps=-1 (max grad steps), learning_rate = 5e-5, Steps 1e7,
Reward is growing very slowly with high std, peak reached 19.8, but mean is around 15.5

Continued training for 3e6 steps. Very slow reward growth. Peak at 20.0, average is approximately same
